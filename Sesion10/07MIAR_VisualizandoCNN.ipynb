{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenMena/07MIAR_RedesNeu_DeepLearning/blob/main/07MIAR_VisualizandoCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdC5lWJwwhLC"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZCivOAkNHRK"
      },
      "source": [
        "Para finalizar con la última práctica del curso vamos a intentar entender un poco más en profundidad que ocurre dentro de una CNN. Para conseguirlo debemos saber que existen dos cosas fundamentales que podemos visualizar:\n",
        "\n",
        "-  **Los mapas de activaciones a la salida de las capas.** Son simplemente los resultados que obtenemos a la salida de una determinada capa durante el *forward pass*. Normalmente, cuando visualizamos las activaciones de una red con activaciones de tipo ReLU, necesitamos unas cuantas épocas antes de empezar a ver algo útil. Una cosa para la que son muy útiles es para ver si algún filtro está completamente negro para diferentes entradas, es decir, todos sus elementos son siempre 0. Esto significa que el filtro está muerto, y normalmente pasa cuando entrenamos con learning rates altos.\n",
        "\n",
        "- **Los filtros aprendidos de los bloques convolucionales**. Normalmente, estos filtros son más interpetables en las primeras capas de la red que en las últimas. Sobre todo, es útil visualizar los filtros de la primera, que está mirando directamente a las imágenes de entrada. Una red bien entrenada tendrá filtros perfectamente definidos, al menos en las primeras capas, y sin practicamente ruido. Si por el contrario tuviésemos filtros con mucho ruido podría deberse a que hace falta entrenar más la red, o a que tenemos overfitting y necesitamos algún método de regularización.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSQLujE6PLE5"
      },
      "source": [
        "A continuación vamos a llevar a cabo un ejemplo para poner todo lo anterior en práctica. **En primer lugar**, vamos a ver cómo se pueden **visualizar las activaciones de la última capa convolucional de nuestra CNN**. Para ello, utilizaremos una técnica llamada Grad-CAM (Gradient Class Activation Map). La idea que hay detrás es bastante sencilla: para averiguar la importancia de una determinada clase en nuestro modelo, simplemente tomamos su gradiente con respecto a la capa convolucional final y luego lo sopesamos con la salida de esta capa.\n",
        "\n",
        "Este es el esquema de uso de Grad-CAM:\n",
        "\n",
        "1) Calcular la salida del modelo y la salida de la última capa convolucional para la imagen en la que queremos calcular el mapa de activación (se puede sacar en otras capas convolucionales).\n",
        "\n",
        "2) Encontrar el índice de la clase que ha predicho el modelo dada la imagen.\n",
        "\n",
        "3) Calcular el gradiente de la clase predicha con respecto a la última capa convolucional.\n",
        "\n",
        "4) Promediarlo y luego ponderarlo con la última capa convolucional (multiplicarlos).\n",
        "\n",
        "5) Normalizar entre 0 y 1 para la visualización.\n",
        "\n",
        "6) Convertir a RGB y superponerlo a la imagen original."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librerías necesarias\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from vis.utils import utils\n",
        "from tensorflow.keras import activations"
      ],
      "metadata": {
        "id": "Gm4HkjAd_CTY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "d9077d97-bbe0-4b73-b215-cd529e3d970b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6de5035f88e0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vis'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construimos el modelo, en este caso VGG16 con los pesos de imagenet\n",
        "model = VGG16(weights='imagenet', include_top=True)\n",
        "# Compilamos el modelo\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Visualizamos el modelo\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "nkkavGpg_EiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7lb7CdtydQ9"
      },
      "outputs": [],
      "source": [
        "# Hacemos los imports necesarios para calcular los Grand-Cams\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops.gradients_impl import image_grad\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow # cv2.imshow does not work on Google Colab notebooks, which is why we are using cv2_imshow instead\n",
        "\n",
        "\n",
        "def Grad_CAMs (img, model, layer, INTENSITY):\n",
        "  # Procesamos la imagen\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "  # Calculamos los gradientes de la capa de la cual queremos sacar la activación\n",
        "  with tf.GradientTape() as tape:\n",
        "    #Extraemos la capa para la cual estamos interesados en obtener el mapa de activación\n",
        "    last_conv_layer = model.get_layer(layer)\n",
        "    # Construimos los modelos que vamos a utilizar (desde inputs hasta outputs y desde inputs hasta la última capa convolucional)\n",
        "    iterate = tf.keras.models.Model([model.inputs], [model.output, last_conv_layer.output])\n",
        "    model_out, last_conv_layer = iterate(x)\n",
        "    # Obtenemos la clase predicha apra la muestra dada\n",
        "    class_predict= np.argmax(model_out[0])\n",
        "    class_out = model_out[:,class_predict]\n",
        "    #Calculamos los gradientes\n",
        "    grads = tape.gradient(class_out, last_conv_layer) \n",
        "    #promedio de gradientes\n",
        "    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "  # Construimos el mapa de calor\n",
        "  heatmap = tf.reduce_mean(tf.multiply(pooled_grads, last_conv_layer), axis=-1)\n",
        "  heatmap = np.maximum(heatmap, 0)\n",
        "  heatmap /= np.max(heatmap)\n",
        "  heatmap = heatmap.reshape((14, 14))\n",
        "  # Hacemos un resize del mapa de calor al tamaño de la imagen y lo multiplicamos por la imagen\n",
        "  heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "  heatmap = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
        "  image_grad = heatmap * INTENSITY + img\n",
        "  return heatmap, image_grad, class_predict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e04qY-L1QpMj"
      },
      "source": [
        "Acto seguido **cargamos un par de imágenes** sobre las que vamos a ver los mapas de activaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj3_n1cgFw78"
      },
      "outputs": [],
      "source": [
        "# imports necesarios\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (18, 6) # tamaño de las imágenes\n",
        "\n",
        "# Cargamos dos imágenes\n",
        "img1 = utils.load_img('https://image.ibb.co/ma90yJ/ouzel2.jpg', target_size=(224, 224))\n",
        "img2 = utils.load_img('https://image.ibb.co/djhyky/ouzel1.jpg', target_size=(224, 224))\n",
        "\n",
        "# Las mostramos\n",
        "f, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(img1)\n",
        "ax[0].grid(False)\n",
        "ax[0].axis('off')\n",
        "ax[1].imshow(img2)\n",
        "ax[1].grid(False)\n",
        "ax[1].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upGEMjZQQ7vy"
      },
      "source": [
        "A la función `Grad_CAMs` tenemos que pasarle el **modelo**,  **la capa** para la que queremos ver las activaciones, **la imagen** para la que queremos ver las activaciones y **la intensidad** con la que queremos ponderar el mapa de activaciones.\n",
        "    \n",
        "En este caso, la clase para que las que queremos ver las activaciones es la clase predicha (aunuqe nosotros también le podríamos introducir el id de la clase que queremos ver las activaciones. Y qué es eso del id de la clase para la que queremos ver las activaciones? Pues que en el caso de la VGG16 con los pesos de la ImageNet, la clase pájaro es la 20, por lo cual, si le metemos una imagen de un pájaro, debería activarse bastante, e indicarnos en qué se fija para decidir que efectivamente es un pájaro. Si le metiésemos un 64, buscaría una green mamba, que es una serpiente por lo que las activaciones deberían ser mucho menores.\n",
        "\n",
        "\n",
        "![Paj_serp](https://drive.google.com/uc?id=1RcJ2tFw4_lLtwTHK8xUl0QpsMTK1-MF2)\n",
        "\n",
        "Para conocer el listado completo de las 1000 clases de ImageNet con sus correspondiente IDs haced click en el siguiente enlace: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj0tSny1GT-6"
      },
      "outputs": [],
      "source": [
        "# Llamamos a la función Grad_CAMs y le pasamos las imágenes correspondientes, el modelo, la capa para la que queremos calcular el mapa de activación y la intensidad\n",
        "heatmap_1, imagen_1, class_out_1 = Grad_CAMs (img1, model,'block5_conv3', 0.5)\n",
        "heatmap_2, imagen_2, class_out_2 = Grad_CAMs (img2, model,'block5_conv3', 0.5)\n",
        "\n",
        "print(\"Clase predicha para la primera imagen:\" + str(class_out_1))\n",
        "cv2_imshow(imagen_1)\n",
        "print(\"Clase predicha para la segunda imagen:\" + str(class_out_2))\n",
        "cv2_imshow(imagen_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KYyEqt3cVYm"
      },
      "source": [
        "¿Qué os ha parecido esto?¿En qué **partes del pájaro** está prestando **atención** la **CNN** para tomar una decisión y **clasificar**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3-YSJc0XVt-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvRqGkLOwhkl"
      },
      "source": [
        "Una vez vistos los mapas de activación, vamos a proceder a ver distintos **filtros de diferentes capas**. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos las funciones necesarias:"
      ],
      "metadata": {
        "id": "Ef1X2o-cAK2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos algunas funciones que utilizaremos después\n",
        "import tensorflow as tf\n",
        "def create_image ():\n",
        "  return tf.random.uniform((224,224,3), minval=-0.5, maxval=0.5)\n",
        "\n",
        "def plot_image (image, title= 'random'):\n",
        "  image = image - tf.math.reduce_min(image)\n",
        "  image = image / tf.math.reduce_max(image)\n",
        "  plt.imshow(image)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "sK5K3a0zAdHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZL4NalDIMnG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# Creamos la función que nos permiti´ra visualizar los filtros de cualquier capa\n",
        "def visualize_filter (layer_name, f_index= None, iters=50, conv=1):\n",
        "  #Creamos el submodelo hasta la capa que le hayamos indicado\n",
        "  submodel= tf.keras.models.Model (model.input, model.get_layer(layer_name).output)\n",
        "  num_filters=submodel.output.shape[-1]\n",
        "  if f_index is None:\n",
        "    f_index= random_randint(0, num_filters -1)\n",
        "  assert num_filters> f_index, \"f_index is out of bounds\"\n",
        "  image= create_image()\n",
        "  verbose_step = int(iters/10)\n",
        "  #Calculamos los gradientes\n",
        "  for i in range(0, iters):\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(image)\n",
        "      if conv==1:\n",
        "      # Para capas convolucionales\n",
        "        out =submodel(tf.expand_dims(image, axis=0))[:,:,:, f_index]\n",
        "      else:\n",
        "      #Para capas lineales\n",
        "        out =submodel(tf.expand_dims(image, axis=0))[:, f_index]\n",
        "      loss = tf.math.reduce_mean(out)\n",
        "    grads= tape.gradient (loss, image)\n",
        "    grads = tf.math.l2_normalize(grads)\n",
        "    image += grads *10\n",
        "\n",
        "    #if (i + 1) % verbose_step ==0:\n",
        "      #print(f' Iteration: {i +1}, Loss: {loss.numpy(): .4f}')\n",
        "  plot_image(image, f'{layer_name}, {f_index}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5GGaqcSYAdn"
      },
      "source": [
        "\n",
        "En primer lugar, vamos a ver qué filtros se utilizan para detectar a los pájaros y serpientes anteriores:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb17iltqIcHG"
      },
      "outputs": [],
      "source": [
        "from vis.utils import utils\n",
        "from tensorflow.keras import activations\n",
        "# Extraemos la última capa. Podemos cambiar la función de softmax a lineal para realizar una mejor visualización\n",
        "model.layers[-1].activation = activations.linear\n",
        "model = utils.apply_modifications(model)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7va5p3-zPjiE"
      },
      "outputs": [],
      "source": [
        "#Visualizamos los filtros que se utilizan para la clasificación de pájaros\n",
        "layer_name ='predictions'\n",
        "visualize_filter(layer_name, f_index=20, iters=500, conv=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos ahora los patrones que permiten detectar la serpiente (green mamba, índice 64 de ImageNet)\n",
        "layer_name ='predictions'\n",
        "visualize_filter(layer_name, f_index=64, iters=500, conv=0)"
      ],
      "metadata": {
        "id": "IIlr4s1JBQjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, vamos a ver las visualizaciones de algunos de los filtros de la primera capa convolucional:"
      ],
      "metadata": {
        "id": "4FvIjNu8BUJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_name ='block1_conv1'\n",
        "filtros =[2,4,20]\n",
        "for i in filtros:\n",
        "  visualize_filter(layer_name, f_index=i, iters=500, conv=1)"
      ],
      "metadata": {
        "id": "rR36LK3rBTTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a ver ahora filtros de diferentes capas:"
      ],
      "metadata": {
        "id": "GI4Y5GyGBWaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "selected_indices = []\n",
        "for layer_name in ['block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3']:\n",
        "    layer_idx = utils.find_layer_idx(model, layer_name)\n",
        "    # Seleccionamos aleatoriamente 4 filtros de cada capa\n",
        "    submodel= tf.keras.models.Model (model.input, model.get_layer(layer_name).output)\n",
        "    num_filters=submodel.output.shape[-1]\n",
        "    filters = np.random.permutation(num_filters)[:4]\n",
        "\n",
        "    # Generamos el mapa de activaciones\n",
        "    vis_images = []\n",
        "    for idx in filters:\n",
        "        visualize_filter(layer_name, f_index=idx, iters=500, conv=1)"
      ],
      "metadata": {
        "id": "b0tmvYJsBZpA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}